Metadata-Version: 2.4
Name: verl
Version: 0.4.0.dev0
Summary: verl: Volcano Engine Reinforcement Learning for LLM
Home-page: https://github.com/volcengine/verl
Author: Bytedance - Seed - MLSys
Author-email: zhangchi.usc1992@bytedance.com, gmsheng@connect.hku.hk
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: accelerate
Requires-Dist: codetiming
Requires-Dist: datasets
Requires-Dist: dill
Requires-Dist: hydra-core
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: peft
Requires-Dist: pyarrow>=19.0.0
Requires-Dist: pybind11
Requires-Dist: pylatexenc
Requires-Dist: ray[default]>=2.41.0
Requires-Dist: torchdata
Requires-Dist: tensordict<=0.6.2
Requires-Dist: transformers
Requires-Dist: wandb
Requires-Dist: packaging>=20.0
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pre-commit; extra == "test"
Requires-Dist: py-spy; extra == "test"
Provides-Extra: prime
Requires-Dist: pyext; extra == "prime"
Provides-Extra: geo
Requires-Dist: mathruler; extra == "geo"
Provides-Extra: gpu
Requires-Dist: liger-kernel; extra == "gpu"
Requires-Dist: flash-attn; extra == "gpu"
Provides-Extra: math
Requires-Dist: math-verify; extra == "math"
Provides-Extra: vllm
Requires-Dist: tensordict<=0.6.2; extra == "vllm"
Requires-Dist: vllm<=0.8.5; extra == "vllm"
Provides-Extra: sglang
Requires-Dist: tensordict<=0.6.2; extra == "sglang"
Requires-Dist: sglang[openai,srt]==0.4.6.post5; extra == "sglang"
Requires-Dist: torch-memory-saver>=0.0.5; extra == "sglang"
Requires-Dist: torch==2.6.0; extra == "sglang"
Provides-Extra: trl
Requires-Dist: trl<=0.9.6; extra == "trl"
Dynamic: author
Dynamic: author-email
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist

<h2 align="center">EDGE-GRPO
</a>

<h5 align="center">
<div align="center">

[Xingjian Zhang](https://scholar.google.com/citations?user=H34fwioAAAAJ&hl=zh-CN)<sup>1*</sup>,
[Siwei Wen](https://scholar.google.com/citations?user=kJRiUYwAAAAJ&hl=zh-CN)<sup>1,2*</sup>,
[Wenjun Wu](https://iai.buaa.edu.cn/info/1013/1093.htm)<sup>1,2,3</sup>, 
[Lei Huang](https://huangleibuaa.github.io/)<sup>1,2,3,‚úâ</sup>

<sup>1</sup>SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China<br>
<sup>2</sup>Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Beihang University, <br>
<sup>3</sup>Hangzhou International Innovation Institute, Beihang University, Hangzhou, China

</div>

<div align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2507.21848-AD1C18.svg?logo=arXiv)](https://arxiv.org/abs/2507.21848)
[![Huggingface](https://img.shields.io/badge/ü§ó-%20Open%20In%20HF-blue.svg)](https://huggingface.co/collections/Zhang199/edge-grpo-688974025917352b5e335752)
[![GitHub issues](https://img.shields.io/github/issues/ZhangXJ199/EDGE-GRPO?color=critical&label=Issues)](https://github.com/ZhangXJ199/EDGE-GRPO)
[![GitHub Stars](https://img.shields.io/github/stars/ZhangXJ199/EDGE-GRPO?style=social)](https://github.com/ZhangXJ199/EDGE-GRPO)

</div>

## üì∞ News

- [2025-07] üéâ Our arXiv paper [EDGE-GRPO](https://arxiv.org/abs/2507.21848) is released!

## <img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/256/2435/2435606.png"> About

Large Language Models have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal. In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the **EDGE-GRPO algorithm**, which adopts **E**ntropy-**D**riven Advantage and **G**uided **E**rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach.

<div align="center">
<img src="figure/framework.png" alt="framework" width="90%" height="auto">
</div>

## üõ†Ô∏è Installation

1. Clone this repository and navigate to the folder
```bash
git clone https://github.com/ZhangXJ199/EDGE-GRPO.git
cd EDGE-GRPO
```

2. Create a conda environment, activate it and install Packages
```Shell
conda create -n edge_grpo python=3.10 -y
conda activate edge_grpo
pip install -r requirements.txt
```

## üìå Usage

### Trained Model

The model we provided after training: [EDGE-GRPO-Qwen-7B](https://huggingface.co/Zhang199/EDGE-GRPO-Qwen-7B), [EDGE-GRPO-Qwen-1.5B](https://huggingface.co/Zhang199/EDGE-GRPO-Qwen-1.5B)

### Train

Replace model paths and output_dir with yours in `train_grpo.sh`

```bash
bash train_grpo.sh
```

### Evaluation

```bash
python evaluate_model.py --model_name YOUR_MODEL_PATH
```

## üìä Results

Performance comparison of different methods on three benchmarks during training steps. Our method consistently outperforms the vanilla GRPO and the variant with forced reflection throughout the training process.

<div align="center">
<img src="figure/comparison_during_training.png" alt="framework" width="100%" height="auto">
</div>

Pass@1 performance comparison across various mathematical evaluation benchmarks. The results below are from 1 epoch of training on DeepScaleR-Hard-1K. The number of samples in each benchmark is indicated in parentheses. The results are evaluated under the setting of temperature = 0.1. The best results are indicated by **boldface**.

<div align="center">
<img src="figure/main_result.jpg" alt="framework" width="90%" height="auto">
</div>

## <img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/256/3557/3557963.png">Changes in Entropy and Advantage Variance During Training

This figure compares the training dynamics of three methods‚ÄîVanilla GRPO, GRPO + Forced Reflection, and EDGE-GRPO (Ours)‚Äîwith respect to two key metrics:

- **Left (Mean Entropy)**: Our method maintains consistently higher policy entropy during training, indicating stronger exploration ability and response diversity, which helps prevent premature convergence.

- **Right (Advantages Variance)**: EDGE-GRPO significantly outperforms the baselines by preserving higher intra-group advantage variance, effectively mitigating the advantage collapse problem and providing more informative gradient signals.
<div align="center">
<img src="figure/changes.png" alt="framework" width="100%" height="auto">
</div>

## üìù Citation

If you find our work interesting and helpful, please consider giving our repo a star. Additionally, if you would like to cite our work, please use the following format:
```bibtex
@article{zhang2025edge,
  title={EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity},
  author={Zhang, Xingjian and Wen, Siwei and Wu, Wenjun and Huang, Lei},
  journal={arXiv preprint arXiv:2507.21848},
  year={2025}
}
```

## üì® Contact

If you have any questions or suggestions, please feel free to contact us at ``zhangxingjian@buaa.edu.cn``.

## ‚ù§Ô∏è Community efforts

* This repository is based on [trl](https://github.com/huggingface/trl) project.
* The implementation of evaluation refers to the [understand-r1-zero](https://github.com/sail-sg/understand-r1-zero) project. Great work!
